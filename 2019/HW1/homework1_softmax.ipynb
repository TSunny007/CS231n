{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework1_softmax.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qcUwZ2kYYV-q"
      },
      "source": [
        "# Homework 1: Classifiers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i2fakcGhJ08N"
      },
      "source": [
        "# Linear Softmax Classifier\n",
        "\n",
        "This exercise is analogous to the SVM exercise. You will:\n",
        "\n",
        "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
        "- implement the fully-vectorized expression for its **analytic gradient**\n",
        "- **check your implementation** with numerical gradient\n",
        "- use a validation set to **tune the learning rate and regularization** strength\n",
        "- **optimize** the loss function with **SGD**\n",
        "- **visualize** the final learned weights\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z3CGTpwFJ08P",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AGrRij4Tqdxf"
      },
      "source": [
        "## Load and preprocess CIFAR-10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EpJ-YmcDJ08S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "8b272324-7ae5-49f2-e406-b26cdf18aee3"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "def get_CIFAR10_data(num_training=49000, num_validation=1000,\n",
        "                     num_test=1000, num_dev=500):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the linear classifier. These are the same steps as we used for the\n",
        "    SVM, but condensed to a single function.  \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    \n",
        "    # All the data comes in the uint8 format, so we need to convert\n",
        "    # it to floats so that we compute numbers greater than 255.\n",
        "    X_train = X_train.astype(np.float)\n",
        "    X_test = X_test.astype(np.float)\n",
        "    # Also, for convenience we flatten the class arrays.\n",
        "    y_train = y_train.flatten()\n",
        "    y_test = y_test.flatten()\n",
        "    \n",
        "    # Split the data into train, val, and test sets. In addition we will\n",
        "    # create a small development set as a subset of the training data;\n",
        "    # we can use this for development so our code runs faster.\n",
        "    \n",
        "    # Our validation set will be num_validation points from the original\n",
        "    # training set.\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    \n",
        "    # Our training set will be the first num_train points from the original\n",
        "    # training set.\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    \n",
        "    # We will also make a development set, which is a small subset of\n",
        "    # the training set.\n",
        "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "    X_dev = X_train[mask]\n",
        "    y_dev = y_train[mask]\n",
        "    \n",
        "    # We use the first num_test points of the original test set as our\n",
        "    # test set.\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    \n",
        "    # Preprocessing: reshape the image data into rows\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "    \n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image = np.mean(X_train, axis = 0)\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "    X_dev -= mean_image\n",
        "    \n",
        "    # third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
        "    # only has to worry about optimizing a single weight matrix W.\n",
        "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
        "\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)\n",
        "print('dev labels shape: ', y_dev.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train data shape:  (49000, 3073)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 3073)\n",
            "Validation labels shape:  (1000,)\n",
            "Test data shape:  (1000, 3073)\n",
            "Test labels shape:  (1000,)\n",
            "dev data shape:  (500, 3073)\n",
            "dev labels shape:  (500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1YqRGKxjJ08V"
      },
      "source": [
        "## Define a naive Softmax classifier loss function\n",
        "\n",
        "Next we define the Softmax loss function.  This will be a naive implementation using loops.  Most of the code for this loss function already exists, but you will need to write code of your own to finish it.  Follow the instructions in the TODO section.\n",
        "\n",
        "Recall that the contribution of a training point $(x_i, y_i)$ to the Softmax loss function is\n",
        "\n",
        "$$L_i = -\\log \\left( \\frac{\\exp(s_{y_i})}{\\sum_{j} \\exp(s_j)} \\right)$$\n",
        "\n",
        "This is the cross-entropy between the predicted class probabilities, and the distribution with all probability concentrated at $y_i$.  The score $s$ is again parametrized by a linear function $s_j = xW_j$ where $x$ is a single data sample and $W_j$ is the $j$th column of $W$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNzBpdmbY67R",
        "colab": {}
      },
      "source": [
        "def softmax_loss_naive(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Softmax loss function, naive implementation (with loops)\n",
        "\n",
        "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
        "    of N examples.\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "        that X[i] has label c, where 0 <= c < C.\n",
        "    - reg: (float) regularization strength\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "    # regularization!                                                           #\n",
        "    #############################################################################\n",
        "    N = X.shape[0]\n",
        "    C = W.shape[1]\n",
        "  \n",
        "    for i in range(N):\n",
        "        predictions = X[i].dot(W)\n",
        "        # http://cs231n.github.io/linear-classify/#softmax\n",
        "        shift_scores = predictions  - np.max(predictions) # this is so that we avoid number overflow with large numbers\n",
        "        # before simplifying: - np.log(np.exp(shift_scores[y[i]])) + np.log(np.sum(np.exp(shift_scores)))\n",
        "        loss_i = - shift_scores[y[i]] + np.log(np.sum(np.exp(shift_scores)))\n",
        "        loss += loss_i\n",
        "        for j in range(C):\n",
        "            softmax_output = np.exp(shift_scores[j])/ np.sum(np.exp(shift_scores)) \n",
        "            if j == y[i]:\n",
        "                dW[:,j] += (-1 + softmax_output) * X[i].T \n",
        "            else:\n",
        "                dW[:,j] += softmax_output * X[i]\n",
        "    loss /= N\n",
        "    loss += 0.5 * reg * np.sum( W * W )\n",
        "    dW = dW/N + reg * W\n",
        "    #############################################################################\n",
        "    #                                                    END OF YOUR CODE       #\n",
        "    #############################################################################\n",
        "\n",
        "    return loss, dW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IJPp2yuBJ08W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d77c2a72-df83-4885-cabc-f8665e3bfc3c"
      },
      "source": [
        "# Evaluate the naive implementation of the loss we provided for you:\n",
        "import time\n",
        "\n",
        "# Generate a random softmax weight matrix and use it to compute the loss.\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
        "print('loss: %f' % loss)\n",
        "print('sanity check: %f' % (-np.log(0.1)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 2.338894\n",
            "sanity check: 2.302585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OSZOfaTpJ08Z"
      },
      "source": [
        "## Inline Question 1:\n",
        "Why do we expect our loss to be close to -log(0.1)? Explain briefly.\n",
        "\n",
        "**Your answer:** *When randomly assigned, the chance of predicting the correct label is $\\frac{1}{N} = 0.1$*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vaIHaDRiZTP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "e4c3cb7d-dd8b-42df-b317-dbe0eb8b4ced"
      },
      "source": [
        "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
        "    \"\"\"\n",
        "    sample a few random elements and only return numerical\n",
        "    in this dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(num_checks):\n",
        "        ix = tuple([np.random.randint(m) for m in x.shape])\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h # increment by h\n",
        "        fxph = f(x) # evaluate f(x + h)\n",
        "        x[ix] = oldval - h # increment by h\n",
        "        fxmh = f(x) # evaluate f(x - h)\n",
        "        x[ix] = oldval # reset\n",
        "\n",
        "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
        "        grad_analytic = analytic_grad[ix]\n",
        "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
        "        print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
        "    \n",
        "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
        "# version of the gradient that uses nested loops.\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
        "# The numeric gradient should be close to the analytic gradient.\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
        "\n",
        "# similar to SVM case, do another gradient check with regularization\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numerical: 2.486768 analytic: 2.486768, relative error: 2.811273e-08\n",
            "numerical: 0.404401 analytic: 0.404401, relative error: 1.540451e-07\n",
            "numerical: -0.272207 analytic: -0.272207, relative error: 1.932183e-08\n",
            "numerical: 0.637583 analytic: 0.637583, relative error: 5.803165e-08\n",
            "numerical: 0.888231 analytic: 0.888231, relative error: 1.578729e-08\n",
            "numerical: 0.849247 analytic: 0.849247, relative error: 6.063271e-08\n",
            "numerical: -0.646503 analytic: -0.646503, relative error: 1.932111e-08\n",
            "numerical: -0.245525 analytic: -0.245525, relative error: 1.123638e-07\n",
            "numerical: 0.444421 analytic: 0.444421, relative error: 7.746457e-08\n",
            "numerical: -2.648947 analytic: -2.648947, relative error: 1.150285e-08\n",
            "numerical: 2.946625 analytic: 2.946625, relative error: 1.142355e-08\n",
            "numerical: 2.533303 analytic: 2.533303, relative error: 1.391059e-08\n",
            "numerical: -0.583699 analytic: -0.583700, relative error: 1.336934e-07\n",
            "numerical: -1.777149 analytic: -1.777149, relative error: 4.517478e-09\n",
            "numerical: -0.071806 analytic: -0.071806, relative error: 3.351410e-07\n",
            "numerical: -4.863885 analytic: -4.863884, relative error: 3.688734e-09\n",
            "numerical: -1.538437 analytic: -1.538437, relative error: 5.772061e-09\n",
            "numerical: 4.173578 analytic: 4.173578, relative error: 2.079408e-10\n",
            "numerical: -0.589330 analytic: -0.589330, relative error: 1.252252e-07\n",
            "numerical: 3.296634 analytic: 3.296634, relative error: 1.187357e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AW-ddDOCsXMW"
      },
      "source": [
        "## Define a vectorized Softmax classifier loss function\n",
        "\n",
        "Next we define the vectorized (i.e. no loops) version of the Softmax loss function.  Most of the code for this loss function already exists, but you will need to write code of your own to finish it.  Follow the instructions in the TODO section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D2HDgzaMZfRV",
        "colab": {}
      },
      "source": [
        "def softmax_loss_vectorized(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Softmax loss function, vectorized version.\n",
        "    Inputs:\n",
        "      - W: A numpy array of shape (D, C) containing weights.\n",
        "      - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "      - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "          that X[i] has label c, where 0 <= c < C.\n",
        "      - reg: (float) regularization strength\n",
        "\n",
        "    Inputs and outputs are the same as softmax_loss_naive.\n",
        "    \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "    # regularization!                                                           #\n",
        "    #############################################################################\n",
        "    N = X.shape[0]\n",
        "    C = W.shape[1]\n",
        "    \n",
        "    scores = X.dot(W)\n",
        "    scores_shifted = scores - np.max(scores, axis=1)[:, np.newaxis]\n",
        "    scores_shifted = np.exp(scores_shifted)\n",
        "    scores_shifted = scores_shifted/np.sum(scores_shifted, axis=1)[:, np.newaxis] # (N, C)\n",
        "    loss = np.mean(-np.log(scores_shifted[np.arange(N), y])) + 0.5 * reg * np.sum( W * W )\n",
        "    \n",
        "    scores_shifted[np.arange(N), y] -= 1\n",
        "    dW = X.T.dot(scores_shifted)/N + reg * W\n",
        "    #############################################################################\n",
        "    #                                                    END OF YOUR CODE       #\n",
        "    #############################################################################\n",
        "\n",
        "    return loss, dW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qSxdTSBPJ08d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "072b83fb-ebfe-43c7-da45-8fb650699b17"
      },
      "source": [
        "# Evaluate the naive implementation of the Softmax gradients\n",
        "\n",
        "# The naive implementation and the vectorized implementation should match, but\n",
        "# the vectorized version should still be much faster.\n",
        "tic = time.time()\n",
        "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
        "\n",
        "tic = time.time()\n",
        "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
        "\n",
        "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
        "# of the gradient.\n",
        "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
        "print('Gradient difference: %f' % grad_difference)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naive loss: 2.338894e+00 computed in 0.218840s\n",
            "vectorized loss: 2.338894e+00 computed in 0.010896s\n",
            "Loss difference: 0.000000\n",
            "Gradient difference: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c6zzNjOgtYDq"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "We now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss. Follow the instructions in the TODO sections below.  You may just want to copy the code you wrote for the SVM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FRZYRkF7ZzE0",
        "colab": {}
      },
      "source": [
        "class Softmax(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
        "                        batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "            training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "            means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - reg: (float) regularization strength.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        num_train, dim = X.shape\n",
        "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Sample batch_size elements from the training data and their           #\n",
        "            # corresponding labels to use in this round of gradient descent.        #\n",
        "            # Store the data in X_batch and their corresponding labels in           #\n",
        "            # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
        "            # and y_batch should have shape (batch_size,)                           #\n",
        "            #                                                                       #                                                                      #\n",
        "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "            # replacement is faster than sampling without replacement.               #\n",
        "            #########################################################################\n",
        "            batch_indices = np.random.choice(num_train, batch_size, replace=False)\n",
        "            X_batch = X[batch_indices]\n",
        "            y_batch = y[batch_indices]\n",
        "            #########################################################################\n",
        "            #                                             END OF YOUR CODE          #\n",
        "            #########################################################################\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Update the weights using the gradient and the learning rate.          #\n",
        "            #########################################################################\n",
        "            self.W -= learning_rate * grad\n",
        "            #########################################################################\n",
        "            #                                             END OF YOUR CODE          #\n",
        "            #########################################################################\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "            training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "            array of length N, and each element is an integer giving the predicted\n",
        "            class.\n",
        "        \"\"\"\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        ###########################################################################\n",
        "        # TODO:                                                                   #\n",
        "        # Implement this method. Store the predicted labels in y_pred.            #\n",
        "        ###########################################################################\n",
        "        y_pred = np.argmax(X.dot(self.W), axis=1)\n",
        "        ###########################################################################\n",
        "        #                                                     END OF YOUR CODE    #\n",
        "        ###########################################################################\n",
        "        return y_pred\n",
        "    \n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative. \n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "            data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "        - reg: (float) regularization strength.\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \"\"\"\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH1ZAx9-BswE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "769a41e8-2747-44eb-a411-d8f0f922016c"
      },
      "source": [
        "softmax = Softmax()\n",
        "tic = time.time()\n",
        "loss_hist = softmax.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4, num_iters=1500, verbose=True)\n",
        "toc = time.time()\n",
        "print('That took %fs' % (toc - tic))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 / 1500: loss 394.424965\n",
            "iteration 100 / 1500: loss 237.758080\n",
            "iteration 200 / 1500: loss 144.623902\n",
            "iteration 300 / 1500: loss 88.235126\n",
            "iteration 400 / 1500: loss 54.174305\n",
            "iteration 500 / 1500: loss 33.517193\n",
            "iteration 600 / 1500: loss 21.047487\n",
            "iteration 700 / 1500: loss 13.674594\n",
            "iteration 800 / 1500: loss 8.951198\n",
            "iteration 900 / 1500: loss 6.281711\n",
            "iteration 1000 / 1500: loss 4.522940\n",
            "iteration 1100 / 1500: loss 3.584903\n",
            "iteration 1200 / 1500: loss 2.976872\n",
            "iteration 1300 / 1500: loss 2.576789\n",
            "iteration 1400 / 1500: loss 2.328329\n",
            "That took 11.366863s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOiYikTdBobX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "45befb9d-9d81-4968-ef17-c2f997729701"
      },
      "source": [
        "# A useful debugging strategy is to plot the loss as a function of\n",
        "# iteration number:\n",
        "plt.plot(loss_hist)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAESCAYAAADXMlMiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP0DQuQsafC23AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KOKw3bKKJ08f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "2a3fac32-1da4-43cf-951c-d6bcfdf34852"
      },
      "source": [
        "from itertools import product\n",
        "# Use the validation set to tune hyperparameters (regularization strength and\n",
        "# learning rate). You should experiment with different ranges for the learning\n",
        "# rates and regularization strengths; if you are careful you should be able to\n",
        "# get a classification accuracy of over 0.35 on the validation set.\n",
        "results = {}\n",
        "best_val = -1\n",
        "best_softmax = None\n",
        "learning_rates = [1e-7, 5e-7]\n",
        "regularization_strengths = [2.5e4, 5e4]\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Use the validation set to set the learning rate and regularization strength. #\n",
        "# This should be identical to the validation that you did for the SVM; save    #\n",
        "# the best trained softmax classifer in best_softmax.                          #\n",
        "################################################################################\n",
        "for l_rate, reg_strength in product(learning_rates, regularization_strengths):\n",
        "    softmax = Softmax()\n",
        "    loss_hist = softmax.train(X_train, y_train, learning_rate=l_rate, reg=reg_strength, num_iters=1500)\n",
        "    train_accuracy = np.mean(y_train == softmax.predict(X_train))\n",
        "    val_accuracy = np.mean(y_val == softmax.predict(X_val))\n",
        "    \n",
        "    results[(l_rate,reg_strength)] = (train_accuracy, val_accuracy)\n",
        "    if val_accuracy > best_val:\n",
        "        best_val = val_accuracy\n",
        "        best_softmax = softmax           \n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "    \n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "    \n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.351592 val accuracy: 0.368000\n",
            "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.324714 val accuracy: 0.343000\n",
            "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.352490 val accuracy: 0.375000\n",
            "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.317286 val accuracy: 0.327000\n",
            "best validation accuracy achieved during cross-validation: 0.375000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rY18wElRJ08i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5d7eba87-4151-417f-e4fc-1e6edba3461b"
      },
      "source": [
        "# evaluate on test set\n",
        "# Evaluate the best softmax on test set\n",
        "y_test_pred = best_softmax.predict(X_test)\n",
        "test_accuracy = np.mean(y_test == y_test_pred)\n",
        "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax on raw pixels final test set accuracy: 0.359000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-iwERfXwJ08l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "586ae6b8-31a5-4fd2-c223-0df0834c03b7"
      },
      "source": [
        "# Visualize the learned weights for each class\n",
        "w = best_softmax.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    \n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADiCAYAAABeKzy5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7EFiDVPKejJy"
      },
      "source": [
        "*Many thanks to Stanford CS231n for permission to use their materials!*"
      ]
    }
  ]
}